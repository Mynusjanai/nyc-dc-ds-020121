{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3 Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- http://localhost:8888/notebooks/Documents/Flatiron/Repository/nyc-dc-ds-020121/Phase_3/ds-gradient_descent-main/gradient_descent.ipynb\n",
    "- Loss functions\n",
    "- Step Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- http://localhost:8888/notebooks/Documents/Flatiron/Repository/nyc-dc-ds-020121/Phase_3/ds-logistic_regression-master/logistic_regression.ipynb\n",
    "- Logit (target)\n",
    "- Predicting a categorical response\n",
    "- Logistic regression can predict on binary or dichotomous variables (0 or 1)\n",
    "- Interpretation\n",
    "- Odds\n",
    "- .predict() vs .predict_proba()\n",
    "- Cost functions and solutions to the optimization problem\n",
    "- Interpreting coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation & Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- http://localhost:8888/notebooks/Documents/Flatiron/Repository/nyc-dc-ds-020121/Phase_3/ds-regularization-vg-021121/regularization.ipynb\n",
    "- Cross validation\n",
    "- Dealing with overfitting models\n",
    "- Bias-Variance tradeoff\n",
    "- Under vs overfitting models\n",
    "- Ridge, Lasso, Elastic Net: \n",
    "- **Lasso = L1 Regularization = Absolute Value**\n",
    "- **Ridge = L2 Regularization = Squared Value** - Preferred\n",
    "<br><br>\n",
    "- Overfit models overestimate the relevance that predictors have for a target. Thus overfit models tend to have overly large coefficients. Generally, overfitting models come from a result of high model variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- http://localhost:8888/notebooks/Documents/Flatiron/Repository/nyc-dc-ds-020121/Phase_3/ds-naive_bayes/naive_bayes.ipynb\n",
    "- The Naive Assumption = $P(A,B) = P(A\\cap B) = P(A)\\ P(B)$ only if independent \n",
    "- Calculation of Likelihoods\n",
    "- Calculating posteriors\n",
    "- Calculating priors and likelihoods\n",
    "- GaussianNB & MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest-Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- http://localhost:8888/notebooks/Documents/Flatiron/Repository/nyc-dc-ds-020121/Phase_3/ds-k-nearest_neighbors-main/knn.ipynb\n",
    "- Import Statement: from sklearn.neighbors import KNeighborsClassifier, NearestNeighbors\n",
    "- Validation Split\n",
    "- The importance of scaling\n",
    "- K and the Bias-Variance tradeoff\n",
    "- Small K = low bias, high variance\n",
    "- Larger K = high bias, low variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Metrics & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- http://localhost:8888/notebooks/Documents/Flatiron/Repository/nyc-dc-ds-020121/Phase_3/evaluationmetrics/Classification_Evaluation%20.ipynb\n",
    "- Classification accuracy is the easiest classification metric to understand But, it does not tell you the underlying distribution of response values. And, it does not tell you what \"types\" of errors your classifier is making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- http://localhost:8888/notebooks/Documents/Flatiron/Repository/nyc-dc-ds-020121/Phase_3/ds-classification_metrics-master/Classification%20Metrics.ipynb\n",
    "- Classification Report\n",
    "- Cost Matrix\n",
    "- Multiclass Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy** = $\\frac{TP + TN}{TP + TN + FP + FN}$\n",
    "\n",
    "In words: How often did my model correctly identify transactions (fraudulent or not fraudulent)? This should give us the same value as we got from the `.score()` method.\n",
    "- Pro: Takes into account both false positives and false negatives.\n",
    "- Con: Can be misleadingly high when there is a significant class imbalance. (A lottery-ticket predictor that *always* predicts a loser will be highly accurate.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recall** = **Sensitivity** = $\\frac{TP}{TP + FN}$\n",
    "\n",
    "In words: How many of the actually fraudulent transactions did my model identify? \n",
    "- Maximize recall\n",
    "- True Positive Rate\n",
    "- Sensitive to detecting positive instances\n",
    "- Pro: Highly sensitive to false negatives.\n",
    "- Con: No sensitivity to false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision** = $\\frac{TP}{TP + FP}$\n",
    "\n",
    "In words: How often was my model's prediction of 'fraudulent' (positives) correct?\n",
    "- Caring more about the false positives\n",
    "- Rate you identify positives correctly\n",
    "- Pro: Highly sensitive to false positives.\n",
    "- Con: No sensitivity to false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F-1 Score** = $\\frac{2PrRc}{Pr + Rc}$ = $\\frac{2TP}{2TP + FP + FN}$\n",
    "\n",
    "The F score is a combination of precision and recall, which can be useful when both are important for a business problem. Most common is the **F-1 Score**, which is an equal balance of the two using a [harmonic mean](https://en.wikipedia.org/wiki/Harmonic_mean).\n",
    "\n",
    "- Between the Recall and the Precision\n",
    "- Harmonic mean of recall and precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Specificity** = $\\frac{TN}{FP + TN}$\n",
    "- How good is your model at identifying true negatives?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**False Positive Rate** = $\\frac{FP}{TN + FP}$\n",
    "\n",
    "- When the actual value is negative, how often is the prediction incorrect?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](metricstable.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- http://localhost:8888/notebooks/Documents/Flatiron/Repository/nyc-dc-ds-020121/Phase_3/ds-decision_trees-main/decision_tree_modeling.ipynb\n",
    "- Import Statement: from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, plot_tree\n",
    "- Partitioning the dataset\n",
    "- Entropy/Information Gain & Gini\n",
    "- Entropy of a split\n",
    "- Decision Trees for Regressor Problems\n",
    "- Bias & Variance with tree models\n",
    "- Pruning parameters (hyperparameters)\n",
    "- Feature Importances\n",
    "<br><br>\n",
    "\n",
    "**Pros:**\n",
    "- are easy to interpret and to visualize;\n",
    "- can easily capture non-linear patterns;\n",
    "- require little data preprocessing from the user. For example, there is no need to normalize columns;\n",
    "- can be used for feature engineering such as predicting missing values, suitable for variable selection;\n",
    "- make no assumptions about distribution, because of the non-parametric nature of the algorithm;\n",
    "<br><br>\n",
    "\n",
    "**Cons:**\n",
    "- Prone to overfitting\n",
    "- Are sensitive to noisy data. This problem can be significantly ameliorated by ensemble methods.\n",
    "- produce high-biased models with imbalanced datasets (so it is recommended that you balance out the dataset before creating the decision tree).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameters:**\n",
    "<br>\n",
    "- A $k$-nearest-neighbors algorithm allows us to make:\n",
    "1. a 1-nearest-neighbor model;\n",
    "2. a 2-nearest-neighbors model;\n",
    "3. a 3-nearest-neighbors model, etc.\n",
    "<br><br>\n",
    "\n",
    "- Or, for another example, the decision tree algorithm allows us to make:\n",
    "1. a classifier that branches according to information gain;\n",
    "2. a classifier that branches according to Gini impurity;\n",
    "3. a regressor that branches according to mean squared error, etc.\n",
    "<br><br>\n",
    "\n",
    "- Difference of Parametric and Non-Parametric Models\n",
    "- Trying different models and implementation examples\n",
    "- Grid Search & automatically searching\n",
    "- Pipelines (incl. grid search within pipelines)\n",
    "- Data leakage with grid search (fit the scaler within the pipeline)\n",
    "- Randomized Search with Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Averaging\n",
    "- Averaging using VotingClassifier()\n",
    "- Weighted Averaging\n",
    "- Bagging:\n",
    "    - Take a sample of your X_train and fit a decision tree to it.\n",
    "    - Replace the first batch of data and repeat.\n",
    "    - When you've got as many trees as you like, make use of all your individual trees' predictions to come up with some holistic prediction. \n",
    "    - (Most obviously, we could take the average of our predictions, but there are other methods we might try.)\n",
    "    * Because we're resampling our data with replacement, we're *bootstrapping*.\n",
    "    * Because we're making use of our many samples' predictions, we're *aggregating*.\n",
    "    * Because we're bootstrapping and aggregating all in the same algorithm, we're *bagging*.\n",
    "    <br>\n",
    "- Random Forests:\n",
    "    - Pros:\n",
    "        - Super friend! \n",
    "        - High performance \n",
    "        - low variance\n",
    "        - Transparent\n",
    "            - inherited from Decision Trees\n",
    "    \n",
    "    - Cons:\n",
    "        - We got so many trees to plant...\n",
    "        - Computationally expensive\n",
    "        - Memory\n",
    "            - all trees stored in memory\n",
    "            - think back to k-Nearest Neighbors\n",
    "- Breed Variety of Trees\n",
    "- Investigating Forests and Feature Importance\n",
    "- Extremely Randomized Trees (Extra Trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- http://localhost:8888/notebooks/Documents/Flatiron/Repository/nyc-dc-ds-020121/Phase_3/classimbalance/Class_imbalance.ipynb\n",
    "- Handling Class Imbalance\n",
    "- Resampling\n",
    "- Undersampling (downsampling)\n",
    "- Oversampling (upsampling)\n",
    "- Concatinating the samples after over/undersampling\n",
    "- SMOTE (over)\n",
    "- Tomek Links (under)\n",
    "- Cost-sensitive training by penalizing the algorithms by weighing the prediction values. (class weight = 'balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import Statements: \n",
    "    - from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor, GradientBoostingClassifier\n",
    "    - import xgboost\n",
    "- http://localhost:8888/notebooks/Documents/Flatiron/Repository/nyc-dc-ds-020121/Phase_3/ds-boosting-main/boosting_illustration.ipynb\n",
    "- http://localhost:8888/notebooks/Documents/Flatiron/Repository/nyc-dc-ds-020121/Phase_3/ds-boosting-main/boosting_in_sklearn.ipynb\n",
    "- Boosting with sklearn and XGBoost\n",
    "- The fundamental idea of boosting is to use a sequence of weak learners to build a model.\n",
    "- Main uses are GradientBoostingClassifier (also GradientBoostingRegressor) and XGBoost\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference between Algorithms and Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A machine learning algorithm runs on data to create a model. KNN, Decision Trees, linear & logistic regression are examples of algorithms.\n",
    "\n",
    "Models are the output of the machine learning algorithms that are then run on data, after being trained and learned on by the algorithm.\n",
    "\n",
    "The algorithms have outputs such as coeffecients (linear reg), and conditionals (decision trees).\n",
    "\n",
    "Models give you the actual end-result scores showing the performance of what the algorithm learned from the training data to make preds on the test/holdout/unknown data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
