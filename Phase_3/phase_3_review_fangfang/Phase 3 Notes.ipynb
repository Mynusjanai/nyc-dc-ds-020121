{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3 Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- http://localhost:8888/notebooks/Documents/Flatiron/Repository/nyc-dc-ds-020121/Phase_3/ds-gradient_descent-main/gradient_descent.ipynb\n",
    "- Loss functions\n",
    "- Step Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- http://localhost:8888/notebooks/Documents/Flatiron/Repository/nyc-dc-ds-020121/Phase_3/ds-logistic_regression-master/logistic_regression.ipynb\n",
    "- Logit (target)\n",
    "- Predicting a categorical response\n",
    "- Interpretation\n",
    "- Odds\n",
    "- .predict() vs .predict_proba()\n",
    "- Cost functions and solutions to the optimization problem\n",
    "- Interpreting coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation & Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- http://localhost:8888/notebooks/Documents/Flatiron/Repository/nyc-dc-ds-020121/Phase_3/ds-regularization-vg-021121/regularization.ipynb\n",
    "- Cross validation\n",
    "- Dealing with overfitting models\n",
    "- Bias-Variance tradeoff\n",
    "- Under vs overfitting models\n",
    "- Ridge, Lasso, Elastic Net: \n",
    "- **Lasso = L1 Regularization = Absolute Value**\n",
    "- **Ridge = L2 Regularization = Squared Value** - Preferred\n",
    "<br><br>\n",
    "- Overfit models overestimate the relevance that predictors have for a target. Thus overfit models tend to have overly large coefficients. Generally, overfitting models come from a result of high model variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- http://localhost:8888/notebooks/Documents/Flatiron/Repository/nyc-dc-ds-020121/Phase_3/ds-naive_bayes/naive_bayes.ipynb\n",
    "- The Naive Assumption = $P(A,B) = P(A\\cap B) = P(A)\\ P(B)$ only if independent \n",
    "- Calculation of Likelihoods\n",
    "- Calculating posteriors\n",
    "- Calculating priors and likelihoods\n",
    "- GaussianNB & MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest-Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- http://localhost:8888/notebooks/Documents/Flatiron/Repository/nyc-dc-ds-020121/Phase_3/ds-k-nearest_neighbors-main/knn.ipynb\n",
    "- Import Statement: from sklearn.neighbors import KNeighborsClassifier, NearestNeighbors\n",
    "- Validation Split\n",
    "- The importance of scaling\n",
    "- K and the Bias-Variance tradeoff\n",
    "- Small K = low bias, high variance\n",
    "- Larger K = high bias, low variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Metrics & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- http://localhost:8888/notebooks/Documents/Flatiron/Repository/nyc-dc-ds-020121/Phase_3/evaluationmetrics/Classification_Evaluation%20.ipynb\n",
    "- Classification accuracy is the easiest classification metric to understand But, it does not tell you the underlying distribution of response values. And, it does not tell you what \"types\" of errors your classifier is making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- http://localhost:8888/notebooks/Documents/Flatiron/Repository/nyc-dc-ds-020121/Phase_3/ds-classification_metrics-master/Classification%20Metrics.ipynb\n",
    "- Classification Report\n",
    "- Cost Matrix\n",
    "- Multiclass Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy** = $\\frac{TP + TN}{TP + TN + FP + FN}$\n",
    "\n",
    "In words: How often did my model correctly identify transactions (fraudulent or not fraudulent)? This should give us the same value as we got from the `.score()` method.\n",
    "- Pro: Takes into account both false positives and false negatives.\n",
    "- Con: Can be misleadingly high when there is a significant class imbalance. (A lottery-ticket predictor that *always* predicts a loser will be highly accurate.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recall** = **Sensitivity** = $\\frac{TP}{TP + FN}$\n",
    "\n",
    "In words: How many of the actually fraudulent transactions did my model identify? \n",
    "- Maximize recall\n",
    "- True Positive Rate\n",
    "- Sensitive to detecting positive instances\n",
    "- Pro: Highly sensitive to false negatives.\n",
    "- Con: No sensitivity to false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision** = $\\frac{TP}{TP + FP}$\n",
    "\n",
    "In words: How often was my model's prediction of 'fraudulent' (positives) correct?\n",
    "- Caring more about the false positives\n",
    "- Rate you identify positives correctly\n",
    "- Pro: Highly sensitive to false positives.\n",
    "- Con: No sensitivity to false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F-1 Score** = $\\frac{2PrRc}{Pr + Rc}$ = $\\frac{2TP}{2TP + FP + FN}$\n",
    "\n",
    "The F score is a combination of precision and recall, which can be useful when both are important for a business problem. Most common is the **F-1 Score**, which is an equal balance of the two using a [harmonic mean](https://en.wikipedia.org/wiki/Harmonic_mean).\n",
    "\n",
    "- Between the Recall and the Precision\n",
    "- Harmonic mean of recall and precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Specificity** = $\\frac{TN}{FP + TN}$\n",
    "- How good is your model at identifying true negatives?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**False Positive Rate** = $\\frac{FP}{TN + FP}$\n",
    "\n",
    "- When the actual value is negative, how often is the prediction incorrect?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](metricstable.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- http://localhost:8888/notebooks/Documents/Flatiron/Repository/nyc-dc-ds-020121/Phase_3/ds-decision_trees-main/decision_tree_modeling.ipynb\n",
    "- Import Statement: from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, plot_tree\n",
    "- Partitioning the dataset\n",
    "- Entropy/Information Gain & Gini\n",
    "- Entropy of a split\n",
    "- Decision Trees for Regressor Problems\n",
    "- Bias & Variance with tree models\n",
    "- Pruning parameters (hyperparameters)\n",
    "- Feature Importances\n",
    "<br><br>\n",
    "\n",
    "**Pros:**\n",
    "- are easy to interpret and to visualize;\n",
    "- can easily capture non-linear patterns;\n",
    "- require little data preprocessing from the user. For example, there is no need to normalize columns;\n",
    "- can be used for feature engineering such as predicting missing values, suitable for variable selection;\n",
    "- make no assumptions about distribution, because of the non-parametric nature of the algorithm;\n",
    "<br><br>\n",
    "\n",
    "**Cons:**\n",
    "- Prone to overfitting\n",
    "- Are sensitive to noisy data. This problem can be significantly ameliorated by ensemble methods.\n",
    "- produce high-biased models with imbalanced datasets (so it is recommended that you balance out the dataset before creating the decision tree).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference between Algorithms and Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A machine learning algorithm runs on data to create a model. KNN, Decision Trees, linear & logistic regression are examples of algorithms.\n",
    "\n",
    "Models are the output of the machine learning algorithms that are then run on data, after being trained and learned on by the algorithm.\n",
    "\n",
    "The algorithms have outputs such as coeffecients (linear reg), and conditionals (decision trees).\n",
    "\n",
    "Models give you the actual end-result scores showing the performance of what the algorithm learned from the training data to make preds on the test/holdout/unknown data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression can predict on binary or dichotomous variables (0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
