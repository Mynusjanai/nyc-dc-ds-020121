{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4 Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - http://localhost:8888/notebooks/Documents/Flatiron/Repository/nyc-dc-ds-020121/Phase_4/ds-principal_component_analysis-main/principal_component_analysis.ipynb\n",
    " - Good for datasets with lots of features (hundreds or thousands) with issues with multicollinearity\n",
    " - Dimensionality (converting / flattening the dimensions of our data)\n",
    " - Correlation and Covariance Matrices\n",
    " - Eigendecomposition (eigenvalues & eigenvectors)\n",
    "     - First eigenvalue captures info from all dimensions (then in decreasing order of amount of variance they capture in the original dataset\n",
    " - Proportion of Variance\n",
    " - First and Second Principal Components\n",
    " - Good to try PCA first if suspect high multicollinearity between our features (ie: before RFE/K-best)\n",
    " - Works best on models with distance-based models (KNN, Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eigenvectors are related to eigenvalues by the following property:** $\\vec{x}$ is an eigenvector of the matrix $A$ if $A\\vec{x} = \\lambda\\vec{x}$, for some eigenvalue $\\lambda$.\n",
    "\n",
    "- Get a vector back with the very same orientation and angle as the one we started with (either lengthening or shortening it)\n",
    "- $A$ has to be squared for this to work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- http://localhost:8888/notebooks/Documents/Flatiron/Repository/nyc-dc-ds-020121/Phase_4/ds-singular_value_decomposition-main/svd.ipynb\n",
    "- Eigendecomposition (only squared matrices have eigendecompositions)\n",
    "- Diagonalization\n",
    "- Image Compression and use cases: https://rpubs.com/Tanmay007/svd\n",
    "- SVD in Python\n",
    "- Relation to PCA\n",
    "- Least Squares and Optimization Problems\n",
    "- $(A^TA)^{-1}A^T$ where A transpose A is squared, so can be inversed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation Engines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- http://localhost:8888/notebooks/Documents/Flatiron/Repository/nyc-dc-ds-020121/Phase_4/ds-recommendation_systems-main/notebooks/recommender_systems.ipynb\n",
    "\n",
    "**Content-based vs Collaborative-filtering Systems**\n",
    "- Recommender systems can be classified along various lines. One fundamental distinction is content-based vs. collaborative-filtering systems. To illustrate this, consider two different strategies: (a) I recommend items to you that are *similar to other items* you've used/bought/read/watched; and (b) I recommend items to you that people *similar to you* have used/bought/read/watched. The first is the content-based strategy; the second is the collaborative-filtering strategy. \n",
    "    - Another distinction drawn is in whether (a) the system uses existing ratings to compute user-user or item-item similarity, or (b) the system uses machine learning techniques to make predictions. Recommenders of the first sort are called **memory-based**; recommenders of the second sort are called **model-based**.\n",
    "- Content Based Systems looks at the cosine of the angle between two items\n",
    "    - $\\cos(\\theta) = \\frac{\\vec{a}\\cdot\\vec{b}}{|\\vec{a}||\\vec{b}|}$\n",
    "- Collaborative Filtering Systems: \n",
    "    - Utility Matrix\n",
    "    - https://en.wikipedia.org/wiki/Collaborative_filtering\n",
    "    - https://towardsdatascience.com/recommendation-systems-models-and-evaluation-84944a84fb8e\n",
    "    - https://dataconomy.com/2015/03/an-introduction-to-recommendation-engines/\n",
    "    \n",
    "**Matrix Factorization**\n",
    "\n",
    "**Alternating Least-Squares (ALS)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- http://localhost:8888/notebooks/Documents/Flatiron/Repository/nyc-dc-ds-020121/Phase_4/ds-clustering-main/notebooks/kmeans_clustering.ipynb\n",
    "- sys import and default plot params to use seaborn in import statements\n",
    "- Looks like a solver to Salesforce type marketing issues\n",
    "\n",
    "**Clustering & Groups**\n",
    "- Measure the distance between every datapoint and between the anchor points\n",
    "- Whichever is the shortest, assign that datapoint to the closest cluster\n",
    "- Find the center of mass for each cluster (reposition anchor points to center mass/centroid)\n",
    "- Refine and re-position clusters based on new anchor points (n times)\n",
    "- After n times of shuffling, you'll reach a point where datapoints no longer change clusters\n",
    "- **Problems with this model** include too much variablility\n",
    "- Goal is to **minimize** sum total of intra-cluster distance and **maximize** sum total of inter-cluster distance\n",
    "\n",
    "**Clustering vs Classification**\n",
    "- Classification we already know the groups and goal is to **predict** class membership\n",
    "- Clustering we **do not** know which groups are in the dataset and we a trying to **identify** the groups\n",
    "- Use clusters as the **target label** in future classification models\n",
    "\n",
    "- K-Means Plotter\n",
    "    - A lot of randomization when setting centroid seeds\n",
    "    - K in K-means is the number of clusters we want to build (default is 8)\n",
    "    - init choices (k-means++, random, ndarray, callable). K-means++ looks to speed up convergence\n",
    "    - n_init (number of times algorithm will be run with different centroid seeds. Final results is best output. default=10)\n",
    "- On Inertia: (aka elbow method) Uses the sum of squared error calculated from each instance of  ùëò  to find the best value of ùëò.\n",
    "    - Fitted K-Means models have _intertia attribute (sum of squared errors of the model)\n",
    "    - Inertia will decrease with more clusters.\n",
    "- Silhouette Coefficient\n",
    "    - Looking for highest silhouette score (bet. 1 & -1)\n",
    "- Connection between PCA and Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- http://localhost:8888/notebooks/Documents/Flatiron/Repository/nyc-dc-ds-020121/Phase_4/ds-clustering-main/notebooks/hierarchical_clustering.ipynb\n",
    "- Agglomerative & Divisive hierarchical clustering\n",
    "- Comparison with K-Means clustering\n",
    "- Types of Clustering (Linkage Types)\n",
    "\n",
    "**Hierarchical Agglomerative Clustering:**\n",
    "- Starting with each datapoint as its own cluster\n",
    "- Bottom-Up approach to clustering\n",
    "- Merging clusters together until we have as many as we want\n",
    "- Dendograms (view horizontal lines as mergers) \n",
    "![dendro](img/dendogram.png)\n",
    "- How the distance between clusters are measured is called the model's **linkage**\n",
    "    - Single Linkage\n",
    "    - Complete Linkage\n",
    "    - Average Linkage\n",
    "    - Ward Linkage\n",
    "    - https://towardsdatascience.com/understanding-the-concept-of-hierarchical-clustering-technique-c6e8243758ec\n",
    "- Cophenetic correlation coeffiecient is the pearson correlation between distances of clusters on the dendrogram\n",
    "    - Cophonetic Matrix\n",
    "    - https://people.revoledu.com/kardi/tutorial/Clustering/Online-Hierarchical-Clustering.html\n",
    "- Building Dendograms with Scipy & Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- http://localhost:8888/notebooks/Documents/Flatiron/Repository/nyc-dc-ds-020121/Phase_4/ds-time_series-main/notebooks/time_series.ipynb\n",
    "- Look at Facebook Prophet for automation of time series\n",
    "- Look at Udemy for deeper learning into time series (use incog window for cheaper prices)\n",
    "- Manipulating Datetime Objects\n",
    "\n",
    "**Endogenous & Exogenous variables**\n",
    "- Endogenous: derived from target variable (y) ie. lag variables for future predictions\n",
    "- Exogenous: extraneous data from the outside used to create variability in the data prediction (new columns ie. holiday, etc.)\n",
    "- Consider the time frames for the exogenous data and upsample/downsample depending on the data preds\n",
    "- Use rolling and weighted averages to inspect time series\n",
    "- Conduct Dickey-Fuller Tests for stationarity (type of hypothesis test)\n",
    "- Load Trend function from Google Trends (pretty cool)\n",
    "    - Visualizing trends\n",
    "- Series as both **predictor** and **target**\n",
    "    - Data and timeseries violates assumptions we hold about regular linear regression\n",
    "- Resampling\n",
    "    - Upsampling: **To upsample** is to increase the frequency of the data of interest\n",
    "    - Downsampling: **To downsample** is to decrease the frequency of the data of interest\n",
    "- pandas.date_range()\n",
    "- Foward Fill: (ffill()) & Back Fill: (bfill())\n",
    "- Interpolation\n",
    "\n",
    "**Visual Diagnostics with Moving Averages**\n",
    "- Simple Moving Average: utilizing the rolling() function. Caluclates a statistic across a moving window. Smooths out the series. Larger the window size, larger the smoothing effect\n",
    "- Exponential Moving Average: gives more weight to data points closer to the date in question. The higher the $\\alpha$ parameter, the closer the EWMA will be to the actual value of the point.\n",
    "        - https://en.wikipedia.org/wiki/Moving_average#Exponential_moving_average\n",
    "        \n",
    "**Components of Time Series Data**\n",
    "- Trend\n",
    "- Seasonality\n",
    "- Cyclical\n",
    "- Irregular\n",
    "\n",
    "**Seasonal Stationarity**\n",
    "- https://stats.stackexchange.com/questions/19715/why-does-a-time-series-have-to-be-stationary\n",
    "![seasonal_stationarity](img/Seasonal_Stationarity.png)\n",
    "\n",
    "**Dickey-Fuller Test**\n",
    "- Hypothesis test (Null & Alternate)\n",
    "- **$NOT$** stationary\n",
    "\n",
    "**Models for Time Series**\n",
    "- Train Test Split is different for Time Seris Models because chronological order matters. Cannot randomly sample points in data, but instead we take a portion of our data at the end and reserve it as our test set\n",
    "- Lags = the features being prior points in the time series (by time units)\n",
    "- Random Walk: predict the next data point by the one before it. Lag from the true data\n",
    "- The Autoregressive Model (AR)\n",
    "    - AIC instead of RMSE. Lower the AIC the better the model. AIC grows as likelihood gets smaller. The model also grows as $k$ grows\n",
    "- The Moving Average Model (MA)\n",
    "    - Based on error. Makes predictions based on how far off we were the day before\n",
    "- ARMA (Auto Regressive Moving Average)\n",
    "    - ACF & PACF: \n",
    "        - PACF helps us choose AR terms\n",
    "        - ACF helps us choose MA terms\n",
    "- ARIMA (Auto Regressive + Moving Average)\n",
    "- SARIMA\n",
    "- SARIMAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebooks**\n",
    "- http://localhost:8888/notebooks/Documents/Flatiron/Repository/nyc-dc-ds-020121/Phase_4/NLP/ds-natural_language_pre-processing-main/natural_language_pre-processing.ipynb\n",
    "- http://localhost:8888/notebooks/Documents/Flatiron/Repository/nyc-dc-ds-020121/Phase_4/NLP/ds-nlp_vectorization-main/nlp_vectorization.ipynb\n",
    "- http://localhost:8888/notebooks/Documents/Flatiron/Repository/nyc-dc-ds-020121/Phase_4/NLP/ds-nlp_modeling-main/nlp_modeling.ipynb\n",
    "\n",
    "**Natural Language Pre-Processing**\n",
    "- Pre-processing notebook\n",
    "- nltk imports for natural language toolkit\n",
    "- re import for Regular Expressions\n",
    "- NLP process map\n",
    "- Collection of text/data is called the **corpus**.\n",
    "- Tokenization\n",
    "    - n-grams\n",
    "    - bigrams\n",
    "- string.punctuation used to remove punctuations\n",
    "\n",
    "**Stemming and Lemmatizing**\n",
    "- NLP Vectorization notebook\n",
    "- Stemming: Porter vs Snowball stemming. Porter is less aggressive in stemmings\n",
    "- Lemmatizing: uses part-of-speech tagging vs the stem/root method of stemming\n",
    "\n",
    "**NLP Feature Engineering**\n",
    "- Bag of Words (BoW)\n",
    "- Vectorization\n",
    "- tf-idf Vectorizer\n",
    "\n",
    "**Naive Bayes and NLP Modeling**\n",
    "- NLP Modeling notebook\n",
    "- Laplace Smoothing\n",
    "    - d: number of features (in this instance total number of vocabulary words)\n",
    "    - ùõº  can be any number greater than 0 (it is usually 1)\n",
    "- **Use for Capstone**\n",
    "    - functions from nlp_modeling notebook\n",
    "- Count Vectorizer\n",
    "    - Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Neural Network Architecture\n",
    "- http://localhost:8888/notebooks/Documents/Flatiron/Repository/nyc-dc-ds-020121/Phase_4/Neural_nets/ds-neural_network_architecture-main/neural_network_architecture.ipynb\n",
    "\n",
    "**Relation to Previous Models**\n",
    "- Logistic Regression: Think of the weights as the betas and the activation function as the sigmoid function\n",
    "- Stacking Ensembles: Various base models' predictions are fed into a \"meta-estimator\" that is trained to aggregate them optimally. This is analogous to the multiple layers of a neural network\n",
    "\n",
    "**Inspiration to Biological Neurons**\n",
    "- Densely connected neurons\n",
    "- Collector Function\n",
    "- Activation Function\n",
    "\n",
    "**Neural Network Construction**\n",
    "- Forward Propagation\n",
    "- Weights\n",
    "- Summation\n",
    "- Activation (activation layers specified in keras models)\n",
    "\n",
    "**Activation Functions**\n",
    "- Sigmoid \n",
    "- Hyperbolic Tangent (tanh)\n",
    "- ReLU\n",
    "- Swish\n",
    "- Softmax\n",
    "\n",
    "#### 2. Keras & Tensorflow\n",
    "- http://localhost:8888/notebooks/Documents/Flatiron/Repository/nyc-dc-ds-020121/Phase_4/Neural_nets/ds-intro_to_keras_and_tensorflow-main/intro_to_keras_and_tf.ipynb\n",
    "\n",
    "**Back Propogation**\n",
    "- Algorithm to send data backwards through the neural network\n",
    "- Computes the gradient of the loss function with respect to the parameters\n",
    "- Not responsible for updating params, that is done by optimization algorithms\n",
    "\n",
    "**Loss Function**\n",
    "- The loss function tells us how well our model performed by comparing the predictions to the actual values.\n",
    "- When we train our models with `keras`, we will watch the loss function's progress across epochs.  A decreasing loss function will show us that our model is **improving**.\n",
    "- The loss function is associated with the nature of our output. In logistic regression, our output was binary, so our loss function was the negative loglikelihood, aka **cross-entropy**.\n",
    "- [Neural networks are much like computational graphs](https://medium.com/tebs-lab/deep-neural-networks-as-computational-graphs-867fcaa56c9)\n",
    "- And computational graphs can be used [to approximate *any* function](http://neuralnetworksanddeeplearning.com/chap4.html). If we're thinking of networks, then, as function approximators, of course we'll want to know how good the approximation is. And so once again we have the idea of a [loss function](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html), which is of course what licenses our thinking of networks as models in the first place. Many loss functions are available. Your choice will depend in part on whether you're doing a regression or classification problem.\n",
    "- Regression:\n",
    "    - mean / median absolute error\n",
    "    - mean / median squared error\n",
    "    - [Huber loss](https://en.wikipedia.org/wiki/Huber_loss)\n",
    "- Classification:\n",
    "    - Crossentropy\n",
    "    - [Hinge loss](https://en.wikipedia.org/wiki/Hinge_loss)\n",
    "    - [Kullback-Leibler divergence](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained)\n",
    "- Resources:\n",
    "    - Here is a good summary of different [loss functions]( https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html):\n",
    "    - For continuous variables, the loss function we have relied on is [MSE or MAE](http://rishy.github.io/ml/2015/07/28/l1-vs-l2-loss/).\n",
    "    - Good [resource](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/) on backpropogation with RMSE loss function.\n",
    "    \n",
    "**Gradient Descent, Epochs, and Batches**\n",
    "- The gradient of the loss function is calculated in relation to each parameter of our neural net.\n",
    "- Can see how model is performing as well as use it to update our parameters\n",
    "- Batch Gradient Descent:\n",
    "    - The gradient is calculated across all values. We can find the direction of the gradient, and proceed directly towards the minimum.\n",
    "    - The weights are updated with regard to the cost at the end of an epoch after all training elements have passed through\n",
    "- Stochastic Gradient Descent:\n",
    "    - Updating the weights after all training examples have passed through can be detrimentally slow.\n",
    "    - SGD updates the weights after each training example. SGD requires fewer epochs to achieve quality coefficients. This speeds up gradient descent significantly.\n",
    "- Mini-Batch Gradient Descent: \n",
    "    - In mini-batch, we pass a batch, calculate the gradient, update the params, then proceed to the next batch. It combines the advantages of batch and stochastic gradient descent: it is faster than SGD since the updates are not made with each point, and more computationally efficient than batch, since we don't have to hold all training examples in memory.\n",
    "    - [Good comparison of types of Gradient Descent and batch size](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/)\n",
    "    \n",
    "**Optimizers**\n",
    "- Adam (Adaptive Moment Estimation):\n",
    "    - A learning rate is maintained for each network weight (parameter) and separately adapted as learning unfolds. See [here](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/).\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
