{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4 Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - http://localhost:8888/notebooks/Documents/Flatiron/Repository/nyc-dc-ds-020121/Phase_4/ds-principal_component_analysis-main/principal_component_analysis.ipynb\n",
    " - Good for datasets with lots of features (hundreds or thousands) with issues with multicollinearity\n",
    " - Dimensionality (converting / flattening the dimensions of our data)\n",
    " - Correlation and Covariance Matrices\n",
    " - Eigendecomposition (eigenvalues & eigenvectors)\n",
    "     - First eigenvalue captures info from all dimensions (then in decreasing order of amount of variance they capture in the original dataset\n",
    " - Proportion of Variance\n",
    " - First and Second Principal Components\n",
    " - Good to try PCA first if suspect high multicollinearity between our features (ie: before RFE/K-best)\n",
    " - Works best on models with distance-based models (KNN, Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eigenvectors are related to eigenvalues by the following property:** $\\vec{x}$ is an eigenvector of the matrix $A$ if $A\\vec{x} = \\lambda\\vec{x}$, for some eigenvalue $\\lambda$.\n",
    "\n",
    "- Get a vector back with the very same orientation and angle as the one we started with (either lengthening or shortening it)\n",
    "- $A$ has to be squared for this to work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- http://localhost:8888/notebooks/Documents/Flatiron/Repository/nyc-dc-ds-020121/Phase_4/ds-singular_value_decomposition-main/svd.ipynb\n",
    "- Eigendecomposition (only squared matrices have eigendecompositions)\n",
    "- Diagonalization\n",
    "- Image Compression and use cases: https://rpubs.com/Tanmay007/svd\n",
    "- SVD in Python\n",
    "- Relation to PCA\n",
    "- Least Squares and Optimization Problems\n",
    "- $(A^TA)^{-1}A^T$ where A transpose A is squared, so can be inversed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation Engines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- http://localhost:8888/notebooks/Documents/Flatiron/Repository/nyc-dc-ds-020121/Phase_4/ds-recommendation_systems-main/notebooks/recommender_systems.ipynb\n",
    "- Content-based vs Collaborative-filtering Systems\n",
    "    - Recommender systems can be classified along various lines. One fundamental distinction is content-based vs. collaborative-filtering systems. To illustrate this, consider two different strategies: (a) I recommend items to you that are *similar to other items* you've used/bought/read/watched; and (b) I recommend items to you that people *similar to you* have used/bought/read/watched. The first is the content-based strategy; the second is the collaborative-filtering strategy. \n",
    "    - Another distinction drawn is in whether (a) the system uses existing ratings to compute user-user or item-item similarity, or (b) the system uses machine learning techniques to make predictions. Recommenders of the first sort are called **memory-based**; recommenders of the second sort are called **model-based**.\n",
    "- Content Based Systems looks at the cosine of the angle between two items\n",
    "    - $\\cos(\\theta) = \\frac{\\vec{a}\\cdot\\vec{b}}{|\\vec{a}||\\vec{b}|}$\n",
    "- Collaborative Filtering Systems: \n",
    "    - Utility Matrix\n",
    "    - https://en.wikipedia.org/wiki/Collaborative_filtering\n",
    "    - https://towardsdatascience.com/recommendation-systems-models-and-evaluation-84944a84fb8e\n",
    "    - https://dataconomy.com/2015/03/an-introduction-to-recommendation-engines/\n",
    "- Matrix Factorization\n",
    "- Alternating Least-Squares (ALS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- http://localhost:8888/notebooks/Documents/Flatiron/Repository/nyc-dc-ds-020121/Phase_4/ds-clustering-main/notebooks/kmeans_clustering.ipynb\n",
    "- sys import and default plot params to use seaborn in import statements\n",
    "- Looks like a solver to Salesforce type marketing issues\n",
    "- Clustering & Groups\n",
    "    - Measure the distance between every datapoint and between the anchor points\n",
    "    - Whichever is the shortest, assign that datapoint to the closest cluster\n",
    "    - Find the center of mass for each cluster (reposition anchor points to center mass/centroid)\n",
    "    - Refine and re-position clusters based on new anchor points (n times)\n",
    "    - After n times of shuffling, you'll reach a point where datapoints no longer change clusters\n",
    "    - **Problems with this model** include too much variablility\n",
    "    - Goal is to **minimize** sum total of intra-cluster distance and **maximize** sum total of inter-cluster distance\n",
    "- Clustering vs Classification\n",
    "    - Classification we already know the groups and goal is to **predict** class membership\n",
    "    - Clustering we **do not** know which groups are in the dataset and we a trying to **identify** the groups\n",
    "    - Use clusters as the **target label** in future classification models\n",
    "- K-Means Plotter\n",
    "    - A lot of randomization when setting centroid seeds\n",
    "    - K in K-means is the number of clusters we want to build (default is 8)\n",
    "    - init choices (k-means++, random, ndarray, callable). K-means++ looks to speed up convergence\n",
    "    - n_init (number of times algorithm will be run with different centroid seeds. Final results is best output. default=10)\n",
    "- On Inertia: (aka elbow method) Uses the sum of squared error calculated from each instance of  ùëò  to find the best value of ùëò.\n",
    "    - Fitted K-Means models have _intertia attribute (sum of squared errors of the model)\n",
    "    - Inertia will decrease with more clusters.\n",
    "- Silhouette Coefficient\n",
    "    - Looking for highest silhouette score (bet. 1 & -1)\n",
    "- Connection between PCA and Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- http://localhost:8888/notebooks/Documents/Flatiron/Repository/nyc-dc-ds-020121/Phase_4/ds-clustering-main/notebooks/hierarchical_clustering.ipynb\n",
    "- Agglomerative & Divisive hierarchical clustering\n",
    "- Comparison with K-Means clustering\n",
    "- Types of Clustering (Linkage Types)\n",
    "- **Hierarchical Agglomerative Clustering:**\n",
    "    - Starting with each datapoint as its own cluster\n",
    "    - Bottom-Up approach to clustering\n",
    "    - Merging clusters together until we have as many as we want\n",
    "    - Dendograms (view horizontal lines as mergers) \n",
    "    ![dendro](img/dendogram.png)\n",
    "- How the distance between clusters are measured is called the model's **linkage**\n",
    "    - Single Linkage\n",
    "    - Complete Linkage\n",
    "    - Average Linkage\n",
    "    - Ward Linkage\n",
    "    - https://towardsdatascience.com/understanding-the-concept-of-hierarchical-clustering-technique-c6e8243758ec\n",
    "- Cophenetic correlation coeffiecient is the pearson correlation between distances of clusters on the dendrogram\n",
    "    - Cophonetic Matrix\n",
    "    - https://people.revoledu.com/kardi/tutorial/Clustering/Online-Hierarchical-Clustering.html\n",
    "- Building Dendograms with Scipy & Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- http://localhost:8888/notebooks/Documents/Flatiron/Repository/nyc-dc-ds-020121/Phase_4/ds-time_series-main/notebooks/time_series.ipynb\n",
    "- Look at Facebook Prophet for automation of time series\n",
    "- Look at Udemy for deeper learning into time series (use incog window for cheaper prices)\n",
    "- Manipulating Datetime Objects\n",
    "- **Endogenous & Exogenous variables**\n",
    "    - Endogenous: derived from target variable (y) ie. lag variables for future predictions\n",
    "    - Exogenous: extraneous data from the outside used to create variability in the data prediction (new columns ie. holiday, etc.)\n",
    "    - Consider the time frames for the exogenous data and upsample/downsample depending on the data preds\n",
    "- Use rolling and weighted averages to inspect time series\n",
    "- Conduct Dickey-Fuller Tests for stationarity (type of hypothesis test)\n",
    "- Load Trend function from Google Trends (pretty cool)\n",
    "    - Visualizing trends\n",
    "- Series as both **predictor** and **target**\n",
    "    - Data and timeseries violates assumptions we hold about regular linear regression\n",
    "- Resampling\n",
    "    - Upsampling: **To upsample** is to increase the frequency of the data of interest\n",
    "    - Downsampling: **To downsample** is to decrease the frequency of the data of interest\n",
    "- pandas.date_range()\n",
    "- Foward Fill: (ffill()) & Back Fill: (bfill())\n",
    "- Interpolation\n",
    "- **Visual Diagnostics with Moving Averages**\n",
    "    - Simple Moving Average: utilizing the rolling() function. Caluclates a statistic across a moving window. Smooths out the series. Larger the window size, larger the smoothing effect\n",
    "    - Exponential Moving Average: gives more weight to data points closer to the date in question. The higher the $\\alpha$ parameter, the closer the EWMA will be to the actual value of the point.\n",
    "        - https://en.wikipedia.org/wiki/Moving_average#Exponential_moving_average\n",
    "- **Components of Time Series Data**\n",
    "    - Trend\n",
    "    - Seasonality\n",
    "    - Cyclical\n",
    "    - Irregular\n",
    "- **Seasonal Stationarity**\n",
    "    - https://stats.stackexchange.com/questions/19715/why-does-a-time-series-have-to-be-stationary\n",
    "    ![seasonal_stationarity](img/Seasonal_Stationarity.png)\n",
    "- **Dickey-Fuller Test**\n",
    "    - Hypothesis test (Null & Alternate)\n",
    "    - **$NOT$** stationary\n",
    "- **Models for Time Series**\n",
    "    - Train Test Split is different for Time Seris Models because chronological order matters. Cannot randomly sample points in data, but instead we take a portion of our data at the end and reserve it as our test set\n",
    "    - Lags = the features being prior points in the time series (by time units)\n",
    "    - Random Walk: predict the next data point by the one before it. Lag from the true data\n",
    "    - The Autoregressive Model (AR)\n",
    "        - AIC instead of RMSE. Lower the AIC the better the model. AIC grows as likelihood gets smaller. The model also grows as $k$ grows\n",
    "    - The Moving Average Model (MA)\n",
    "        - Based on error. Makes predictions based on how far off we were the day before\n",
    "    - ARMA (Auto Regressive Moving Average)\n",
    "        - ACF & PACF: \n",
    "            - PACF helps us choose AR terms\n",
    "            - ACF helps us choose MA terms\n",
    "    - ARIMA (Auto Regressive + Moving Average)\n",
    "    - SARIMA\n",
    "    - SARIMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
